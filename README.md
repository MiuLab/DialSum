# Abstractive Dialogue Summarization with Sentence-Gated Modeling Optimized by Dialogue Acts

## Reference
Main paper to be cited

```
@inproceedings{goo2018abstractive,
  title={Abstractive Dialogue Summarization with Sentence-Gated Modeling Optimized by Dialogue Acts},
    author={Chih-Wen Goo and Yun-Nung Chen},
    booktitle={Proceedings of 7th IEEE Workshop on Spoken Language Technology},
    year={2018}
}
```

## Want to Reproduce the experiment?
Simply run `python3 train.py`.

## Where to Put My Own Dataset?
Use `--data_path=path_to_dataset`.<br>
`path_to_dataset` shoulde includes three folders - train, test, and valid, which is named 'train', 'test', and 'valid'. <br>
Each of these folders contains three files - dialogue sentences, dialogue act label, and summary, which is named 'in', 'da', and 'sum'. <br>
Each line represents an example and input sentences should be seperated by a special `<EOS>` token. <br>
Vocabulary files need to be generated by yourself, including `_PAD` and `_UNK`.<br>

## Requirements
tensorflow 1.4 <br>
python 3.5

## Usage
some sample usage <br>
* run with 128 units full model, and no patience for early stop <br>
&emsp;python3 train.py --num_units=128 --model_type=full --patience=0

* load model and evaluate it <br>
&emsp;python3 train.py --evaluate --ckpt=full_path_to_ckpt

* use "python3 train.py -h" for all avaliable parameter settings

## Note
A bug found in the script of ROUGE computation is fixed, so the scores will be different from the original paper, but the trend of improvement is the same.
